---
title: "American Expenses, Driver Physiology"
author: "Michael Cowan"
format: 
  html:
    embed-resources: true
    df-print: paged
editor: visual
---

## 1. US Education Expenditures

```{r}
library(tidyverse)
library(marginaleffects)
```

**a.**

```{r}
US_spend <- read_csv ("http://ritsokiguess.site/datafiles/Anscombe.csv")
US_spend
```

**b.**

```{r}
spend.1 <- lm(education ~ income + young + urban, data=US_spend)
summary(spend.1)
```

**c.** We would not considering removing any of the explanatory variables from the model fitted because all three variables show statistical significance (p-values all far less than \< .05).

**d.** First we'll create dataframe that holds the values for imaginary state A and state B (called df).

```{r}
df <- data.frame(
  income = c(2800, 3200),
  young = c(370, 350),
  urban = c(790, 650))
df
```

With this dataframe, we'll use cbind(predictions to predict the average (mean) educational level for state A and B -- displaying the predicted mean response, the lower and upper confidence limits, and the values of the explanatory variables (income, young, and urban).

```{r}
cbind(predictions(spend.1, newdata = df)) %>% 
  select(income, young, urban, estimate, conf.low, conf.high)
```

**e.** Given the confidence intervals -- state A \~141-174 (width of \~33), and state B \~180-196 (width of \~16) -- state B would be more accurate as the lower and upper limits are narrower; the limits being closer together suggests a higher level of precision -- that is to say, less uncertainty around the estimated mean value of education expenditure for the set of explanatory variables.

To further understand why these differences might occur, we can compare the specific values of the explanatory variables for state A and state B against the overall mean values of these explanatory variables in the dataset.

```{r}
US_spend %>%
  summarize(across(c(income, young, urban), mean))
```

-   For comparison, state A (income = 2800, young = 370, urban = 790) and state B (income = 3200, young = 350, urban = 650).

Visibly, state B's values are closer to the original dataset's mean values -- for income and urban -- than state A's values, suggesting that state B's values are more representative of the data the model was derived from.

Thus, the model will provide more accurate predictions/narrow confidence interval for state B.

## 2. Driver's Seat Position

**a.**

```{r}
seats <- read_csv ("http://ritsokiguess.site/datafiles/seatpos.csv")
seats
```

**b.** We'll use lm(hipcenter here (specifying "." to containt all the other variables). Interestingly, only the intercept is non-significant.

```{r}
seats.1 <- lm(hipcenter ~., data=seats)
summary(seats.1)
```

**c.** In the model, none of the independent variables (Age, Weight, HtShoes, Ht, Seated, Arm, Thigh, Leg) show statistical significance (p \< 0.05). Given that the overall model is statistically significant (as indicated by the p-value: 1.306e-05), the discrepancy suggests that combining these variables can predict hipcenter significantly better than a model with no predictors, even though no single variable stands out as a significant predictor on its own.

This inconsistency is meaningful given the practical implications; hipcenter still has a substantial non-zero value, but it is likely that all the values would impact the location of a persons hipcenter (though perhaps not individually -- rather, they collectively influence hipcenter it in a combined way that hasn't been properly defined/captured).

In addition, one could argue that the the directionality of some of the coefficients is strange/inconsistent. For example, one might expect a heavier individual to have a lower hipcenter while sitting in a car (one might expect they would sink into the seat). Similarly, height without shoes has a positive coefficient, yet height with shoes has a negative coefficient; one might expect two (likely) highly correlated measures of height would not have coefficients in opposite directions. Likewise, one might expect to see a negative coefficient for age, given expected postural changes/joint stiffness/seating habits as one grows older.

Ultimately, we might be looking at a situation where the variables are highly correlated with one another (multicollinearity) and thus distorting the individual coefficients. Alternatively, perhaps there are variables missing in the model -- for example, posture, or the way individuals adjust their car seats -- that might be crucial in determining hipcenter, but are not captured by the included variables.

**d.**

```{r}
seats.2 <- lm(hipcenter ~HtShoes, data=seats )
summary(seats.2)
```

The first model including multiple predictors has a slightly higher R-squared value compared to the second model with only HtShoes -- indicating that the first model explains a slightly higher portion of the variation in hipcenter.

However, importantly, the higher Adjusted R-squared value in the second model (using only one predictor) suggests that the second model is more efficient and parsimonious at singularly predicting hipcenter without additional variables. Thus, given the simplicity of the second model -- which yielded a substantial significant P-value: 2.207e-09, and HtShoes significant P-value: 2.21e-09 -- it is clear that additional predictors beyond HtShoes do not substantially increase the model's explanatory power.

However, since the question asks for a test, we can compare the 2 models:

```{r}
anova(seats.2, seats.1)
```

There is no significant difference in the fit of the two models, but we'll pick the **seats.2** because of the higher R-squared.

**e.** It is likely that the P-value of HtShoes is different between the two models given that in the first model, the effect of HtShoes is measured while controlling the influence of other variables (Age, Weight, Ht, etc.) -- meaning the coefficient is calculated given only its specific contribution. In the second model, the coefficient reflects the total effect of HtShoes on hipcenter without any adjustment from other variables, meaning there are no other variables that will absorb part of the variation explained by HtShoes. As such, it makes sense that the HtShoes is statistically significant in the second model, rather than the first.

**f.** Based on what we know or can guess about the variables in this dataset, it makes practical sense that we would see the results of part **(b)** and **(e)** due to the predictor variables being highly correlated with one another (multicollinearity) and thus distorting the individual coefficients (as mentioned previously). All the variables included in the first model (one would think) are highly related to each other and have a combined effect on hipcenter -- but not enough individually to demonstrate statistically significance in the first model.

Alternatively, it could be the case that there is an overarching variable missing in the model that better accounts for the complex relationship between body dimensions and hip center than any of these individual predictor variables -- for example, posture, or the way individuals adjust their car seats. I

Ultimately, it is likely the case that these specific, highly-correlated variables are interacting in a complex way that is not being captured by the first model, or they are simply providing redundant information that has diluted the individual significance of each variable.
